# Step 2: ç»§æ‰¿ BaseTrainerï¼Œå†»ç»“ DLï¼Œè®­ç»ƒ RL Agent
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from collections import defaultdict, deque
import os
import time
import pandas as pd
from utils.logger import suppress_console_logging

# å‡è®¾ä½¿ç”¨ tensorboard è®°å½•
from torch.utils.tensorboard import SummaryWriter


class PPOTrainer:
    def __init__(self, config, agent, env, buffer, reward_calculator, train_loader, val_loader=None, logger=None):
        """
        PPO è®­ç»ƒç®¡ç†å™¨
        Args:
            config: å…¨å±€é…ç½®
            agent: PPOAgent å®ä¾‹
            env: TransformerRLEnv å®ä¾‹
            buffer: RolloutBuffer å®ä¾‹
            reward_calculator: RewardCalculator å®ä¾‹
            train_loader: è®­ç»ƒé›† DataLoader (éœ€æ”¯æŒæ— é™è¿­ä»£æˆ–è‡ªåŠ¨é‡ç½®)
            val_loader: éªŒè¯é›† DataLoader
        """
        self.rl_config = config.rl
        self.agent = agent
        self.env = env
        self.buffer = buffer
        self.reward_calc = reward_calculator
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.logger = logger

        self.device = self.rl_config.device
        self.save_dir = self.rl_config.save_dir
        self.writer = SummaryWriter(log_dir=os.path.join(self.save_dir, 'writer'))

        self.total_epochs = self.rl_config.total_epochs
        self.ppo_epochs = self.rl_config.ppo_epochs

        # å°† DataLoader è½¬ä¸ºè¿­ä»£å™¨ï¼Œä»¥ä¾¿åœ¨ RL å¾ªç¯ä¸­æŒ‰éœ€è·å–
        self.train_iter = iter(self.train_loader)
        self.eval_interval = len(self.train_loader.dataset) // self.rl_config.batch_size + 1

        # ç»Ÿä¸€å¯¹æ‰€æœ‰æŒ‡æ ‡ (Loss, Reward, Acc...) è¿›è¡Œæ»‘åŠ¨å¹³å‡
        self.window_metrics = defaultdict(lambda: deque(maxlen=20))  # çª—å£å¤§å°å¯æŒ‰éœ€è°ƒæ•´ï¼Œä¾‹å¦‚ 20 æˆ– 100

        self.history_data = []
        self.csv_path = os.path.join(self.save_dir, 'acc_loss/RL_training_history.csv')

        # è·å–æ··åˆç²¾åº¦é…ç½®
        # å»ºè®®åœ¨ config ä¸­æ·»åŠ è¯¥å­—æ®µï¼Œé»˜è®¤ False
        self.use_amp = getattr(self.rl_config, 'use_amp', True)
        # self.dtype = torch.float16 if self.use_amp else torch.float32
        # ç¡®å®šè®¾å¤‡ç±»å‹ä¾› autocast ä½¿ç”¨ ('cuda' or 'cpu')
        self.device_type = 'cuda' if 'cuda' in str(self.device) else 'cpu'

        self.logger.info(f"Total epochs: {self.total_epochs}, "
                         f"PPO epochs: {self.ppo_epochs}, "
                         f"Eval interval: {self.eval_interval}, "
                         f"Rollout batch size: {self.rl_config.batch_size}, "
                         f"Update batch size: {buffer.batch_size}")

    def _get_batch_data(self):
        """ä» DataLoader è·å–ä¸‹ä¸€ä¸ª Batchï¼Œå¦‚æœè€—å°½åˆ™é‡ç½®"""
        try:
            batch = next(self.train_iter)
        except StopIteration:
            self.train_iter = iter(self.train_loader)
            batch = next(self.train_iter)
        return batch

    def collect_rollouts(self):
        """
        æ”¶é›†ç»éªŒ (Rollout Phase)
        è¿è¡Œ env.reset() -> è·‘ num_steps æ­¥ -> å­˜å…¥ Buffer
        """
        self.buffer.reset()
        self.agent.eval()  # æ”¶é›†æ•°æ®æ—¶ä¸æ›´æ–° BatchNorm

        # 1. è·å–æ–°çš„ä¸€æ‰¹å›¾åƒæ•°æ®å¹¶ Reset ç¯å¢ƒ
        batch_data = self._get_batch_data()

        # å°† Labels ç§»åˆ° Device (Rewardè®¡ç®—éœ€è¦)
        # å‡è®¾ batch_data['labels'] æ˜¯ [B, Total]
        batch_labels = batch_data['labels'].to(self.device)

        # Reset Env (ä¼šæ‰§è¡Œ Pre-rollout)
        with torch.no_grad():
            obs = self.env.reset(batch_data)
        # åœ¨æ¯ä¸ª episode å¼€å§‹æ—¶é‡ç½®çŠ¶æ€ã€‚
        self.reward_calc.reset()

        # ç»Ÿè®¡å½“å‰ Rollout çš„æ€»å¥–åŠ±
        current_ep_reward = torch.zeros(self.env.batch_size, device=self.device)
        current_rollout_stats = {}

        for step in range(self.env.max_steps):
            for k, v in obs.items():
                if torch.isnan(v).any():
                    print(f"Detected NaN in observation: {k}, step {step}")
            with torch.no_grad(), torch.amp.autocast(device_type=self.device_type, enabled=self.use_amp):
                # 1. Agent å†³ç­–
                # return: dict_action, raw_corr, stop, log_prob, value
                action, raw_corr, stop, log_prob, value = self.agent.get_action(obs)

                # 2. Env æ‰§è¡Œ
                next_obs, _, dones, info = self.env.step(action)

                # 3. è®¡ç®— Reward (ä½¿ç”¨ info ä¸­çš„ pre_action_mask)
                # æ³¨æ„ï¼šä¼ å…¥ Env ä¸­é”å®šçš„ final_logits
                rewards, r_info = self.reward_calc.compute_reward(
                    logits=self.env.final_logits.detach(),  # æ˜¾å¼ detach å¢å¼ºå®‰å…¨æ€§,
                    labels=batch_labels,
                    stop_decision=stop,
                    pre_action_mask=info['pre_action_mask'],  # [Key] ä½¿ç”¨æ—§ Mask
                    done_mask=dones
                )

            # è®°å½•ç»Ÿè®¡
            current_ep_reward += rewards

            # 4. å­˜å…¥ Buffer
            # æ³¨æ„ï¼šbuffer éœ€è¦å­˜ raw_correction ç”¨äºåç»­è®¡ç®—åˆ†å¸ƒ
            buffer_action = {'correction': raw_corr, 'stop': stop}
            # ä¿®æ­£å
            # ç¡®ä¿ step_indices æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [Batch] çš„ Tensor
            # å‡è®¾å½“å‰ step å¯¹æ‰€æœ‰ batch æ ·æœ¬éƒ½æ˜¯ä¸€æ ·çš„
            step_indices_tensor = torch.full((self.env.batch_size,), info['step'], dtype=torch.long, device='cpu')

            self.buffer.add(
                obs=obs,
                action=buffer_action,
                log_prob=log_prob,
                reward=rewards,
                done=dones,
                value=value,
                cls_input_q=info['cls_input_q'],
                labels=batch_labels,
                step_indices=step_indices_tensor
            )

            # æ›´æ–° Obs
            obs = next_obs

            # å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½ Done äº†ï¼Œå¯ä»¥æå‰è·³å‡ºå½“å‰ Rollout å¾ªç¯ (å¯é€‰)
            # æˆ–è€…è®© Env å†…éƒ¨å¤„ç† Dummy Step (é€šå¸¸ VectorEnv ä¼šè‡ªåŠ¨ Resetï¼Œä½†è¿™é‡Œæ˜¯å• Batch Env)
            # é‰´äºæˆ‘ä»¬çš„ Env æ˜¯å¤„ç†å›ºå®šæ­¥æ•° (max_steps)ï¼Œè¿™é‡Œå¾ªç¯é€šå¸¸ä¼šè·‘æ»¡ config.num_steps
            # é™¤é max_steps < config.num_stepsï¼Œè¿™é‡Œå‡è®¾ config.num_steps == env.max_steps
            torch.cuda.empty_cache()

            if dones.all():
                break

        # Rollout ç»“æŸï¼Œè®¡ç®— GAE
        # éœ€è¦æœ€åä¸€ä¸ªçŠ¶æ€çš„ Value æ¥åš Bootstrap
        with torch.no_grad():
            _, _, _, last_value = self.agent.network(next_obs)  # åªå– Value

        # ==========================================
        # æ˜¾å¼è°ƒç”¨ Buffer è®¡ç®— Advantage
        # ==========================================
        self.buffer.compute_returns_and_advantage(last_value, dones)

        # --- è®°å½•ç»Ÿè®¡æŒ‡æ ‡ ---
        # 1. Reward
        reward_val = current_ep_reward.mean().item()
        current_rollout_stats['rollout/reward'] = reward_val
        # 2. è§£æ r_info ä¸­çš„æŒ‡æ ‡
        # å‡è®¾ r_info åŒ…å«: {'reward/settled_acc': 0.8, 'reward/settled_f1': 0.7, ...}
        for k, v in r_info.items():
            if 'finalall_' in k:
                metric_name = k.split('finalall_')[-1]
                # åŠ å‰ç¼€åŒºåˆ†ï¼Œä¿æŒ key çš„å”¯ä¸€æ€§
                current_rollout_stats[f'rollout/{metric_name}'] = v

        return current_rollout_stats

    def update(self):
        """
        PPO æ›´æ–° (Update Phase + Joint Cls Training)
        """
        self.agent.train()  # å¼€å¯ Dropout ç­‰

        loss_metrics = defaultdict(list)

        # PPO Epochs (åŒä¸€ä¸ª Batch æ•°æ®æ›´æ–°å¤šæ¬¡)
        for _ in range(self.ppo_epochs):
            data_generator = self.buffer.get_generator()

            for batch in data_generator:
                # Agent å†…éƒ¨æ‰§è¡Œ Forward -> Ratio -> Clip Loss -> Backward
                metrics = self.agent.update(batch)

                for k, v in metrics.items():
                    loss_metrics[k].append(v)

        # å¹³å‡ Loss
        avg_metrics = {k: np.mean(v) for k, v in loss_metrics.items()}
        return avg_metrics

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info(f"Start Training on {self.device}...")

        global_step = 0
        best_F1 = 0.0

        pbar = tqdm(range(self.total_epochs), desc="Training", leave=False)

        for _,epoch in enumerate(pbar):
            torch.cuda.empty_cache()  # é‡Šæ”¾æœªä½¿ç”¨çš„ cached memory
            epoch_log = {'epoch': epoch}
            # 1. æ”¶é›†æ•°æ®
            # print('rollout phase')
            rollout_metrics = self.collect_rollouts()
            # print('rollout phase done')
            # print('update phase')
            epoch_log.update(rollout_metrics)
            # print('update phase done')

            # 2. æ›´æ–°æ¨¡å‹
            train_loss_metrics = self.update()
            epoch_log.update(train_loss_metrics)

            # 3. æ—¥å¿—è®°å½•
            global_step += self.env.max_steps * self.env.batch_size
            # åˆå¹¶æ—¥å¿—é€»è¾‘ï¼šéå† epoch_logï¼Œç»Ÿä¸€æ»‘åŠ¨å¹³å‡
            log_msg_parts = [f"Epoch {epoch}"]

            # å¯¹ epoch_log ä¸­çš„æ‰€æœ‰æ•°å€¼æŒ‡æ ‡åº”ç”¨æ»‘åŠ¨çª—å£
            for k, v in epoch_log.items():
                if k == 'epoch': continue  # ä¸å¹³æ»‘ Epoch æ•°
                if 'update/cls_' in k:
                    if 'update/cls_loss' in k:
                        pass
                    else:
                        continue  # è·³è¿‡ä¸­é—´å˜é‡

                # 1. æ›´æ–°æ»‘åŠ¨çª—å£
                self.window_metrics[k].append(v)

                # 2. è®¡ç®—å¹³æ»‘å€¼
                mean_val = np.mean(self.window_metrics[k])

                # 3. è®°å½•åˆ° TensorBoard (è®°å½•å¹³æ»‘å€¼ï¼Œå‡å°‘æŠ–åŠ¨)
                # å¦‚æœ key å·²ç»åŒ…å«åˆ†ç±»å‰ç¼€(å¦‚ loss/, train_) åˆ™ç›´æ¥ç”¨ï¼Œå¦åˆ™å¯ä»¥åŠ å‰ç¼€
                # è¿™é‡Œå‡è®¾ upstream è¿”å›çš„ key å·²ç»è¶³å¤Ÿæ¸…æ™° (å¦‚ 'loss/total', 'train_reward')
                self.writer.add_scalar(k, mean_val, epoch)

                # 4. è®°å½•åˆ° Console List
                log_msg_parts.append(f"{k}: {mean_val:.4f}")

            # ç»Ÿä¸€æ‰“å°
            pbar.set_postfix_str(' | '.join(log_msg_parts))
            # ğŸ‘‡ è¿™æ®µæ—¥å¿—åªå†™æ–‡ä»¶ï¼Œä¸æ‰“å°åˆ°æ§åˆ¶å°
            with suppress_console_logging(self.logger):
                self.logger.info(' | '.join(log_msg_parts))


            # 4. éªŒè¯é›†è¯„ä¼°
            if (epoch + 1) % self.eval_interval == 0 and self.val_loader:
                val_metrics = self.evaluate(epoch)
                val_log = {f"val_{k}": v for k, v in val_metrics.items()}
                epoch_log.update(val_log)
                is_best = val_metrics['val/f1'] > best_F1
                self.save_checkpoint(epoch, is_best=is_best)

            # 5. ä¿å­˜ CSV (ä¿å­˜çš„æ˜¯å½“æœŸåŸå§‹å€¼ï¼Œä¸æ˜¯å¹³æ»‘å€¼ï¼Œä»¥ä¾¿åç»­åˆ†æçœŸå®æ³¢åŠ¨)
            self.history_data.append(epoch_log)
            df = pd.DataFrame(self.history_data)
            df.to_csv(self.csv_path, index=False)

    def save_checkpoint(self, epoch, is_best=False):
        path = os.path.join(self.save_dir, f"checkpoints/RL_last.pt")
        torch.save({
            'epoch': epoch,
            'agent_state_dict': self.agent.state_dict(),
            # å¦‚æœ classifier æ˜¯å•ç‹¬å¾®è°ƒçš„ï¼Œå®ƒçš„å‚æ•°å˜åŒ–åæ˜ åœ¨ model.class_heads ä¸­
            # è¿™é‡Œä¿å­˜ä¸€ä»½å¼•ç”¨æ–¹ä¾¿æŸ¥çœ‹ï¼Œå®é™…æ¢å¤æ—¶é€šå¸¸åŠ è½½æ•´ä¸ª agent æˆ–åŸå§‹ model
            'classifier_state_dict': self.env.model.class_heads.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'config': self.rl_config,
        }, path)
        self.logger.info(f"Saved checkpoint to {path}")
        if is_best:
            path = os.path.join(self.save_dir, f"checkpoints/RL_best.pt")
            torch.save({
                # 'epoch': epoch,
                'agent_state_dict': self.agent.state_dict(),
                # å¦‚æœ classifier æ˜¯å•ç‹¬å¾®è°ƒçš„ï¼Œå®ƒçš„å‚æ•°å˜åŒ–åæ˜ åœ¨ model.class_heads ä¸­
                # è¿™é‡Œä¿å­˜ä¸€ä»½å¼•ç”¨æ–¹ä¾¿æŸ¥çœ‹ï¼Œå®é™…æ¢å¤æ—¶é€šå¸¸åŠ è½½æ•´ä¸ª agent æˆ–åŸå§‹ model
                'classifier_state_dict': self.env.model.class_heads.state_dict(),
                # 'optimizer_state_dict': self.agent.optimizer.state_dict(),
                'config': self.rl_config,
            }, path)
            self.logger.info(f"Saved best model to {path}")

    def evaluate(self, epoch):
        """éªŒè¯é€»è¾‘"""
        if not self.val_loader:
            return

        self.logger.info(f"Evaluating at epoch {epoch}...")
        self.agent.eval()
        # å¦‚æœ env ä¸­æœ‰è¿™å°±åˆ‡æ¢ï¼Œæ²¡æœ‰å°±ä¸ç®¡ (Transformer backbone é€šå¸¸ä¸€ç›´ eval)
        if hasattr(self.env, 'eval'): self.env.eval()

        # ä½¿ç”¨ defaultdict å­˜å‚¨æ‰€æœ‰å‡ºç°çš„æŒ‡æ ‡
        # Key ä¾‹å¦‚: 'reward', 'steps', 'acc', 'f1', 'precision'...
        val_metrics = defaultdict(list)

        # éå†éªŒè¯é›†
        with torch.no_grad():
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            pbar = tqdm(self.val_loader, desc=f"Eval Ep{epoch}", leave=False)
            for batch_data in pbar:
                with torch.amp.autocast(device_type=self.device_type, enabled=self.use_amp):
                    # æ•°æ®å‡†å¤‡
                    # å‡è®¾ batch_data['labels'] æ˜¯ [B, Total]
                    batch_labels = batch_data['labels'].to(self.device)

                    # Reset Env
                    obs = self.env.reset(batch_data)
                    # åœ¨æ¯ä¸ª episode å¼€å§‹æ—¶é‡ç½®çŠ¶æ€ã€‚
                    self.reward_calc.reset()

                    # ç»Ÿè®¡å®¹å™¨
                    batch_rewards = torch.zeros(self.env.batch_size,device=self.device)
                    steps_taken = torch.zeros(self.env.batch_size, device=self.device)  # è®°å½•æ¯ä¸ªæ ·æœ¬è·‘äº†å¤šå°‘æ­¥
                    active_mask = torch.ones(self.env.batch_size, dtype=torch.bool, device=self.device)  # è®°å½•æ ·æœ¬æ˜¯å¦è¿˜åœ¨è·‘

                    # Rollout loop
                    for step in range(self.env.max_steps):
                        # Agent å†³ç­–
                        action, _, stop, _, _ = self.agent.get_action(obs, deterministic=True)

                        # Env æ‰§è¡Œ
                        next_obs, _, dones, info = self.env.step(action)

                        # 3. è®°å½•æ­¥æ•° (åªè¦è¿˜æ²¡ doneï¼Œæ­¥æ•°å°±+1)
                        # æ³¨æ„ï¼šdones æ˜¯ [B]ï¼Œè¡¨ç¤ºè¯¥æ ·æœ¬æ‰€æœ‰ä»»åŠ¡æ˜¯å¦éƒ½ç»“æŸ
                        # å¦‚æœæ ·æœ¬è¿˜åœ¨è·‘ (active)ï¼Œè¿™ä¸€æ­¥ç®—ä½œæœ‰æ•ˆæ¶ˆè€—
                        steps_taken[active_mask] += 1
                        active_mask = ~dones  # æ›´æ–°æ´»è·ƒçŠ¶æ€

                        # 4. è®¡ç®— Reward (ä»…ä½œè®°å½•)
                        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿¡ä»» RewardCalculator çš„é€»è¾‘ï¼Œä½† Accuracy æˆ‘ä»¬è‡ªå·±ç®—æ›´å‡†
                        # è®¡ç®— Reward (ç”¨äºç»Ÿè®¡æŒ‡æ ‡)
                        rewards, r_info = self.reward_calc.compute_reward(
                            logits=self.env.final_logits.detach(),
                            labels=batch_labels,
                            stop_decision=stop,
                            pre_action_mask=info['pre_action_mask'],
                            done_mask=dones
                        )

                        batch_rewards += rewards
                        obs = next_obs

                        # 5. [ä¼˜åŒ–] æå‰é€€å‡ºï¼šå¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½ç»“æŸäº†ï¼Œä¸éœ€è¦ç©ºè·‘
                        if dones.all():
                            break

                    # --- Batch ç»“ç®— ---

                    # 1. è®°å½•åŸºç¡€æŒ‡æ ‡
                    val_metrics['reward'].append(batch_rewards.mean().item())
                    val_metrics['avg_steps'].append(steps_taken.mean().item())

                    # 2. ä» final_step_info ä¸­æå–é«˜é˜¶æŒ‡æ ‡ (Acc, F1, Rec...)
                    # å‡è®¾ r_info key æ ¼å¼ä¸º "reward/settled_acc" æˆ– "settled_acc"
                    current_acc = 0.0  # ç”¨äºè¿›åº¦æ¡æ˜¾ç¤º
                    current_f1 = 0.0

                    for k, v in r_info.items():
                        # è¿‡æ»¤æ‰ä¸éœ€è¦è®°å½•çš„ä¸­é—´å˜é‡ï¼Œåªä¿ç•™ settled æŒ‡æ ‡
                        if 'finalall_' in k:
                            # æ¸…æ´— key åç§°: 'reward/finalall_acc' -> 'acc'
                            metric_name = k.split('finalall_')[-1]
                            val_metrics[f'val/{metric_name}'].append(v)

                            # é¡ºä¾¿è·å– acc ç”¨äºæ˜¾ç¤º
                            if 'acc' in metric_name:
                                current_acc = v
                            if 'f1' in metric_name:
                                current_f1 = v
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix({
                        'acc': f"{current_acc:.3f}",
                        'f1': f"{current_f1:.3f}",
                        'rew': f"{batch_rewards.mean().item():.2f}"
                    })

            # --- æ±‡æ€»ä¸æ—¥å¿— ---

            # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡çš„å¹³å‡å€¼
            avg_metrics = {k: np.mean(v) for k, v in val_metrics.items()}

            # æ‰“å°å…³é”®ä¿¡æ¯ (ç¡®ä¿ print ä¸ä¼šæŠ¥é”™ï¼Œä½¿ç”¨ get è®¾ç½®é»˜è®¤å€¼)
            self.logger.info(f"Eval Ep {epoch}: "
                             f"Reward={avg_metrics.get('reward', 0):.4f}, "
                             f"Acc={avg_metrics.get('acc', 0):.4f}, "
                             f"F1={avg_metrics.get('f1', 0):.4f}, "
                             f"Steps={avg_metrics.get('avg_steps', 0):.2f}")

            # å†™å…¥ TensorBoard
            for k, v in avg_metrics.items():
                self.writer.add_scalar(f'eval/{k}', v, epoch)

            # æ¢å¤è®­ç»ƒæ¨¡å¼
            self.agent.train()

            return avg_metrics
